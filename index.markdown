---
title: ""
layout: single
classes: wide
author_profile: true
author: ReportCards
---

## Overview

Detailed model evaluation on curated benchmark datasets are stored here to inform users about model biases and accuracy at mapping labels using Allen Institute taxonomies as reference.

## Evaluations: `human SEA-AD` benchmark

Model | Benchmark | Runtime 
--- | --- | --- | --- 
[HANN](Human_reports/HANN_human.md)             | human MTG SEA-AD | 3 Hours 
[HANN (FindMarkers)](Human_reports/HANN_FindMarkers_human.md)             | human MTG SEA-AD | 3 Hours 
[FLAT](Human_reports/FLAT_human.md)             | human MTG SEA-AD | 1.8 Hours 

## Evaluations: `mouse WB` benchmark

Model | Benchmark | Runtime
--- | --- | --- | --- 
[HANN](Mouse_reports/HANN_mouse_WB.md) | mouse WB | 0.76 Hours 
[HANN (FindMarkers)](Mouse_reports/HANN_FindMarkers_mouse_WB.md) | mouse WB | 17.41 Hours 
[FLAT](Mouse_reports/FLAT_mouse_WB.md) | mouse WB | 0.36 Hours 

## Benchmarks
More details about the benchmark data can be found [here](LINK). (Link broken until data cards setup)

## Algorithms
More details about the models benchmarked can be found [here](LINK). (Link broken until model cards setup)

## Scripts
The python scripts to produce the model report cards are hosted on the Allen Institute bmark repo.
